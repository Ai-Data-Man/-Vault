---
{"dg-publish":true,"permalink":"/卡片/为什么在消息队列里消费到了重复的数据1.mp4/","dgPassFrontmatter":true}
---

[为什么在消息队列里消费到了重复的数据1.mp4](file:///Z:%5C我的阅读黑曜石Vault%5C资产库藏%5C互联网Java工程师面试突击训练系列课程%5C第一季%5C07_我的天！我为什么在消息队列里消费到了重复的数据？%5C视频%5C01%5C视频.mp4)
* [[Logseq元问题库/pages/？保证消息不被重复消费？保证消息消费的冥等\|？保证消息不被重复消费？保证消息消费的冥等]]
	* [[Logseq元问题库/pages/？保证消息不被重复消费\|？保证消息不被重复消费]]
		*  消息重复消费并不可怕, 而且[[Logseq元问题库/pages/？保证消息不被重复消费\|？保证消息不被重复消费]] 其实是不可能完全做到的，只能[[Logseq元问题库/pages/？保证消息消费的冥等\|？保证消息消费的冥等]]来应对消息重复消费
			* [[Logseq元问题库/pages/？消息重复消费的情况\|？消息重复消费的情况]]
				* [[Logseq元问题库/pages/？ Kafka 重复消费的情况\|？ Kafka 重复消费的情况]]
					* [03:30](file:///Z:/%5C%E6%88%91%E7%9A%84%E9%98%85%E8%AF%BB%E9%BB%91%E6%9B%9C%E7%9F%B3Vault%5C%E8%B5%84%E4%BA%A7%E5%BA%93%E8%97%8F%5C%E4%BA%92%E8%81%94%E7%BD%91Java%E5%B7%A5%E7%A8%8B%E5%B8%88%E9%9D%A2%E8%AF%95%E7%AA%81%E5%87%BB%E8%AE%AD%E7%BB%83%E7%B3%BB%E5%88%97%E8%AF%BE%E7%A8%8B%5C%E7%AC%AC%E4%B8%80%E5%AD%A3%5C07_%E6%88%91%E7%9A%84%E5%A4%A9%EF%BC%81%E6%88%91%E4%B8%BA%E4%BB%80%E4%B9%88%E5%9C%A8%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E9%87%8C%E6%B6%88%E8%B4%B9%E5%88%B0%E4%BA%86%E9%87%8D%E5%A4%8D%E7%9A%84%E6%95%B0%E6%8D%AE%EF%BC%9F%5C%E8%A7%86%E9%A2%91%5C01%5C%E8%A7%86%E9%A2%91.mp4#t=210.995469)
						* ![图片.png](/img/user/Logseq%E5%85%83%E9%97%AE%E9%A2%98%E5%BA%93/assets/%E5%9B%BE%E7%89%87.png)
						* 这里涉及到 #Kafka/Offset 的概念。 #Kafka 接收到每个消息都会给这个消息标记上 #Kafka/Offset ，代表消息的序号
						* #Consumer 消费了消息后，每隔一段时间, 会将消息最新的 #Kafka/Offset 通过 #Zookeeper 提交给 #Kafka 。这样 #Consumer 重启后， #Kafka 就可以让 #Consumer 从上次消费到的 #Kafka/Offset 处继续消费
					* 但是偏偏可能 #Consumer 刚消费了消息，却因为宕机等原因没有成功提交 #Kafka/Offset ，这样 #Consumer 恢复后重新消费，少数消息就会被重新消费
	* [[Logseq元问题库/pages/？保证消息消费的冥等\|？保证消息消费的冥等]]
---
{"title":"并发总结","url":"https://953a762e.wiz06.com/wapp/pages/view/share/s/2leDoK1ZmQXN2ZIyCG2UoMwX0RAJOC0sRAMT2Rz7sZ2Xn26J","clipped_at":"2022-08-27 20:36:56","tags":["无"],"dg-publish":true,"permalink":"/阅读库藏/并发总结_1661603816/","dgPassFrontmatter":true}
---


# 并发总结

# java后端/java/并发

* [[Logseq元知识库/pages/并发⁰\|并发⁰]]是多个任务彼此干扰的，在统一的时间段内一个个交替着执行。[[Logseq元知识库/pages/并行⁰\|并行⁰]]是多个任务互不干扰的同时执行。[[Logseq元知识库/pages/串行⁰\|串行⁰]]是前一个任务没完成，下一个任务只能等待。
	*   联系：[[Logseq元知识库/pages/并发⁰\|并发⁰]] 和[[Logseq元知识库/pages/并行⁰\|并行⁰]] 都是指处理多个任务·，[[Logseq元知识库/pages/串行⁰\|串行⁰]] 是指处理单个任务
	*   区别：关键看处理任务时间上是否重叠和任务是否干扰；干扰不重叠则叫[[Logseq元知识库/pages/并发⁰\|并发⁰]]，不干扰重叠则叫[[Logseq元知识库/pages/并行⁰\|并行⁰]] ，既不干扰也不重叠叫[[Logseq元知识库/pages/串行⁰\|串行⁰]] 
*   高并发原则  
    *   克服阻塞，尽量少加锁
    *   超时机制，过时不候，减少阻塞时间
    *   轮询机制，可以替代阻塞，但会导致cpu消耗
    *   回调机制，完美
    *   CAS，乐观锁，轻量化锁
*   多线程引入的性能问题  
    *   内核切换
	    *   JVM是KTL架构(线程借由操作系统管理)，所以线程的阻塞、销毁等操作需要调用系统内核级别的api，即cpu需要从用户态切换到内核态，造成性能损失
    *   上下文切换
*   进程、线程、管程  
    *   进程
	    *   粗浅的说，进程是一个运行中的程序。深入来说，进程是计算机分配资源(内存、端口等)的最小单位，进程与进程之间的资源是隔离的。
    *   线程
	    *   cpu执行调度的最小单位。可以认为是一条代码的执行路径，或者程序的一个任务
    *   联系
	    *   进程和线程是一对多，且进程必定至少对应一个线程
	    *   同一个进程下的线程共享进程的资源。每个线程有独立的运行栈和程序计数器(PC)
    *   管程
	    *   管程就是监视器对象，即锁
*   超线程  
    *   是一种让单个物理cpu核可以虚拟出多个逻辑核心的技术
*   [[Logseq元知识库/pages/用户线程\|用户线程]](非[[Logseq元知识库/pages/守护线程\|守护线程]])和 #守护线程  
    *   setDaemon为true的是[[Logseq元知识库/pages/守护线程\|守护线程]]，默认为false即线程默认是[[Logseq元知识库/pages/用户线程\|用户线程]]
    *   [[Logseq元知识库/pages/用户线程\|用户线程]]一挂掉，[[Logseq元知识库/pages/守护线程\|守护线程]]就会自我了结，因为要守护的对象没了。[[Logseq元知识库/pages/用户线程\|用户线程]]是直接面向工作的线程，而[[Logseq元知识库/pages/守护线程\|守护线程]]是默默在幕后做一些系统服务
    *   [[Logseq元知识库/pages/守护线程\|守护线程]]的典型例子就是我们java的gc
*   Future和Callable  
    *   [[Logseq元知识库/pages/Java.Runnable\|Java.Runnable]] 代表一个无需返回值的计算
    *   Callable代表一个需要返回值的计算
    *   Future代表一个异步计算的结果
	    *   get获取：会阻塞，不见不散
	    *   get(timeout): 会阻塞，过时不候
	    *   isDone轮询：
    *   RunnableFuture implement Runnable和Future
    *   FutureTask实现了RunnableFuture, 代表其既可以传给Thread构造，同时可以获得异步计算的结果。FutureTask需要传入一个Callable来构造
*   CompletableFuture  
    *   也实现了Future接口，FutureTask的升级版
    *   继承：  
        *   get同FutureTask
        *   同join区别：唯一区别是join不抛异常
    *   增加以下特性：  
        *   回调（whenComplete，exceptionally
        *   任务编排：依赖((thenApply, 异步计算A依赖异步计算B)、竞争(取最快出来的结果)、等待
*   公平锁与非公平锁：  
    *   公平锁(如RetreenLock(true))
	    *   先到先得FIFO，大家好好排队
    *   非公平锁（如RetreenLock()）
	    *   先占先得,不按序排队
	    *   相对公平锁，劣势在于牺牲了公平，易出现锁饥饿(总是抢不到锁)
	    *   相对于公平锁，优势在于效率
	        *   恢复挂起的线程到真正获取锁在cpu看来还是有可观的时间差的，因此非公平锁可以更充分利用cpu时间片，减少cpu空闲状态
	        *   因为不用判断前驱节点，所以先得到锁的线程在释放锁之后再次得锁的概率变的很大，意味着线程不用切换，减少了开销
    *   差别
	    * ![](/img/user/阅读库藏/assets/1661603816-8b1c83631f090c023ecc4f2b42edfc67.png)
    
    *   选型：如果为了更高吞吐量，非公平锁更合适，因为效率更高；否则就用公平锁
*   可重入锁  
    *   可以允许同一个线程重新进入其同步域的同步锁。比如：
	    *   同步方法递归效果同下图，就好像锁不是同一个一样  
	        ![](/img/user/阅读库藏/assets/1661603816-d61cbd0c6c1723ba374547f5b4725679.png)
        
	    *   可以嵌套重入  
	    ![](/img/user/阅读库藏/assets/1661603816-8212f6dfc464c811565f785f59f7899c.png)
        
	    *   避免死锁
    *   基本实现原理：
	    *   第一次获取锁时计数器状态从0变1，并记录持锁线程，只要计数器状态不为0，其他线程不可以修改计数器状态即获取锁，持锁线程则可以将状态计数器+1然后重新获取锁。释放锁时要将状态计数器-1
*   死锁  
    *   排查工具：
	    *   纯命令工具：jps + jstack
	    *   图形化工具：jconsole
*   多线程中断  
    *   为了安全，应该让线程自己决定自己是否中断，否则随便就被kill了十分不安全，这也就是java废弃了Thread.stop、Thread.suspend、Thread.resume的原因
    *   因为上一点，线程中断的机制必须是协商的
    *   Java 中断api实现的中断机制
	    *   线程内部内置中断标志位(volatile)，初始为false
	    *   Thread类的interupt()在线程正常活动(如果线程已经终 止，就不能设置成功)时可以设置线程的中断标志位(volatile)为true，告诉该线程哥们我希望你停了
    *   线程自己通过isInterupted方法检测中断标志位为true，知道哦有人想叫我停掉，那我来决定停不停
    *   如果线程因sleep，wait，join等方法调用在这些方法上阻塞时，interrupt的调用会让线程退出阻塞抛出interrupted异常(我都已经停了，中断个屁？)并clear中断标志位，如果仍要中断，需要try catch重新interrupt()
    *   interrupted()方法将返回当前标志位，并清除标志位，所以多次调用结果可能不同
    *   我们自己也可以仿照java，通过volatile布尔或者原子布尔，来实现中断
*   并发三大特性  
    *   原子性
    *   可见性
    *   有序性
*   java是用c、c++语言实现的，对应实现源码项目即open jdk。openjdk中还会写一些JNI(java native interface)，它们其实是一些可以调用操作系统底层的c/c++函数方法，当jvm字节码执行器遇到java native方法时便会去调用响应的JNI。JNI和Java API是一一对应的，比如Thread.java会对应一个Thread.c。若执行Thread.start,底层是thread.c调用jvm.cpp,jvm.cpp又调thread.cpp  
    
    ![](/img/user/阅读库藏/assets/1661603816-62a4a2ef5bab6a5f0b5ac92f806fbac1.png)
    
*   Java内存模型(Java Memory Model，简称JMM)
	*   volatile关键字  
		* [[Logseq元知识库/pages/volatile⁰\|volatile⁰]]在原子性上没有保证
		*   通过底层lock前缀指令+充分利用总线锁或MESI协议保证充分的可见性
		    * ![](/img/user/阅读库藏/assets/1661603816-ce4c6ba001f3508fc70f376ffbd4289d.png)
		    *  一次读其实底层是三个命令 read, load, use 
		    * 一次写其实底层是assign,store,write三个命令 
		    *  read-load和store-write必须成对出现 
		    * CPU缓存以缓存行为最小存储单元，一般32-·128字节，也就是主内存加载时一次加载连续的一段，这是基于局部性原理的优化,即如果一个数据被用到，那么其附近的数据也通常会马上被使用。
		    * lock前缀指令，是附加了其他命令作为参数的。
    		    * 第一个作用，就是锁定内存不让其他处理器及其他线程访问，使得整个lock命令变为跨处理器层级的原子操作（不会被其他处理器打断, 也不会被同处理器的其他线程打断）。
        		    * CPU执行lock指令时，会产生一个Lock信号。在Pentium 和早期的 IA-32 处理器中，这个信号总是会导致整个内存总线被锁定，结果就是其他CPU无法通过总线访问内存。在比较新的处理器上，引入了CPU高速缓存，这时再总是锁定总线就属实不智了，因为这样会进一步导致高速缓存不可用（处理器要把数据从主内存复制到高速缓存中），所以现在的处理器只要存在对应高速缓存锁都是只锁对应缓存，锁的粒度减小了，成功规避了原来要锁总线的性能问题，也保证了高速缓存的成功使用。不过因为引入了缓存，就存在一个新的问题，就是缓存一致性，解决整个问题的方案正是MESI协议（缓存一致性协议。M， Modified；E，Exclusive；S，Shared；I，Invalid）。
            		    * 因此如果是lock+总线锁，很轻易的就可以保证可见性，但是性能损耗很大
            		    * 如果是lock+高速缓存锁，就要配合MESI协议来保证可见性
                		    * 每次CPU要**写回**数据(assign-store-write)到主内存时，都确保所有处理器达成了共识，即确保CPU读到(read-load)的数据总是最新的而其他处理器的相同数据失效，这样。共识协商的原理是基于lock指令：在lock附加命令中完成一次共识协商，因为lock前缀的原因，完成这次共识协商将是跨处理器的原子操作。
	                		    * 每次在read-load之前判断一下是否lock 
		                		    * 如果CPU缓存行不存在或者Invalid ，那么就会发生写不命中（write miss），此时处理器就会执行lock指令,，具体共识步骤如下：
			                		    * 将该缓存行锁定并**从主内存加载到CPU缓存中**，之后根据是否有其他处理器也缓存了该缓存行将其状态标记为独占（Exclusive）或共享（Shared)。这一步称为[[Logseq元知识库/pages/读取锁定\|读取锁定]]（read with intent to modify）
			                		    * 并且通过总线嗅探机制（bus snooping）通知其他处理器，使得其他处理器的相同缓存行失效（Invalid）
				                		    * 具体来说
					                		    * [[Logseq元知识库/pages/读取锁定\|读取锁定]] 会向总线发送一个[[Logseq元知识库/pages/RFO\|RFO]]（Read For Ownership）信号以请求读并获得缓存行所有权。因为这个信号是一种用于请求以失效方式读取某个缓存行的总线消息，它会通知其他处理器删除指定内存地址的数据副本（缓存行中的数据）。这个过程称为[[Logseq元知识库/pages/读失效\|读失效]]（read invalidate）
					                		    * [[Logseq元知识库/pages/RFO\|RFO]] 会通过总线嗅探机制通知到其他处理器。
					                		    * 其他处理器收到 [[Logseq元知识库/pages/RFO\|RFO]] 后如果自己也有该缓存行，并且是 Modified状态，么它们必须先将该缓存行**写回**主内存，并将其状态改为 Invalid，这个过程称为 [[Logseq元知识库/pages/写回失效\|写回失效]] (write back invalidate)
			                		    * 处理器接收到其他处理器的失效响应（Invalidate Acknowledge)后lock指令执行就结束了,  处理器接着再进行read-load操作，并将该缓存行的状态标记为修改（Modified），这样处理器后续就可以对缓存进行assign-store- write写操作了
													* 如果CPU缓存是共享（Shared）状态，那么就会发生写命中（write hit），此时处理器也会执行lock指令，具体共识会话步骤如下：
														* 将该缓存行锁定并**写回**主内存，然后标记为 已修改（Modified）。这个过程被称为[[Logseq元知识库/pages/写入锁定\|写入锁定]]（write with invalidate）
														* 并且通过总线嗅探机制（bus snooping）通知其他处理器，使得其他处理器的相同缓存行失效（Invalid）
				                		    * 具体来说
					                		    * [[Logseq元知识库/pages/写入锁定\|写入锁定]] 会向总线发送一个[[Logseq元知识库/pages/IWB\|IWB]]信号以请求写并获得缓存行所有权
					                		    * [[Logseq元知识库/pages/IWB\|IWB]]  会通过总线嗅探机制通知到其他处理器。
					                		    * 其他处理器收到 [[Logseq元知识库/pages/IWB\|IWB]]  后如果自己也有该缓存行，并且是 Modified 或Exclusive  状态，么它们必须先将该缓存行**写回**主内存，并将其状态改为 Invalid，这个过程称为 [[Logseq元知识库/pages/写回失效\|写回失效]] (write back invalidate)					
														 *  当处理器接收到其他处理器的失效响应（Invalidate Acknowledge)后lock指令执行就结束了,  处理器接着再进行read-load操作，并将该缓存行的状态再次标记为修改（Modified），这样处理器后续就可以对缓存进行assign-store- write写操作了
							* 如果CPU缓存是修改（Modified）或独占（Exclusive）状态,那么就不需要执行lock指令，因为这两种状态表示该缓存行只存在于当前处理器中
		      		    * 第二个作用，就是隐性的起到硬件级别的内存屏障作用，禁止CPU把lock之前的读写请求越过lock重排。注意，硬件级别的内存屏障还有sfence，Ifence等，并不是只有lock前缀指令才能起到内存屏障的作用。
		    * lock前缀指令的附加命用CAS函数，为CAS算法保证了可见性与原子性 
	    *   volatile有编译器层面（软件级别）的内存屏障来禁止编译器解释器的指令重排，有处理器层面(硬件级别）的内存屏障【一定有lock，根据处理器架构还有使用其他硬件内存屏障】来禁止cpu的指令重排，以保证一定程度的有序性【因为保证可见性的lock指令，本身也起到内存屏障作用，因此可以说volatile关键字就是靠在处理器每次读取或写入前插入内存屏障，来实现的可见性和有序性】
		    * volatile关键字所实现的重排规则![](/img/user/阅读库藏/assets/1661603816-ed096bd749d584c2c3d8c3edc7bd0c9e.png)
		    * 编译器层面的内存屏障是由Java编译器（javac）或者JIT编译器（HotSpot）插入的，它可以防止编译器对volatile变量相关的指令进行优化重排。根据Java内存模型（JMM）规定，对于volatile变量的写操作，编译器会在前后分别插入一个StoreStore屏障和一个StoreLoad屏障；对于volatile变量的读操作，编译器会在后面插入一个LoadLoad屏障和一个LoadStore屏障。这些屏障可以保证volatile变量在写之前和读之后不会与其他普通变量或者volatile变量混淆顺序
				![](/img/user/阅读库藏/assets/1661603816-eb832790353269951916fb453cb96088.png)![](/img/user/阅读库藏/assets/1661603816-4b20d9f2abd1fcd186d0ddacba6b6420.png)
    
    *   Synchronize关键字
	*   jdk1.6后采用锁升级机制。偏向锁：只有某个线程可以获得的锁。一旦有多个线程抢锁，升级为轻量级锁,用的是自旋CAS抢锁(CAS是一种无锁算法，可实现乐观锁，乐观的含义就是竞争与执行时间比较乐观，这样自己在使用数据时大概率不会碰上并发，所以可以得之我幸，不得也不过是多试几次)；当线程竞争激烈或者同步执行时间过长，便会出现某个线程等很长时间也抢不到锁即自旋过多，这时候就升级为重量级锁(悲观锁，竞争激烈，只能悲观的假定一定会遇上并发问题，所以要真正的上锁)。重量级锁是依靠监视器对象实现的(反编译synchronized字节码会有一个monitorenter和一般两个monitorexit，两个确保一定释放锁。如果同步代码throw了异常，则是一个monitorenter和一个monitorexit和两个athrow,用两个athrow确保异常一定抛出然后让系统中断锁)  
    
    ![](/img/user/阅读库藏/assets/1661603816-7d55525753fd345ae1fd737dbb2f4742.png)
    
    ![](/img/user/阅读库藏/assets/1661603816-18accaeb68d4fdbd27d5d98af6d0ddd5.png)
    
*   标记在方法时，JVM依靠标志位进行自动进入释放monitor  
    
    ![](/img/user/阅读库藏/assets/1661603816-6c4653e5144863c82468f42ca96ea9b2.png)
    
    ![](/img/user/阅读库藏/assets/1661603816-de0615e328d8b2b1778dd43049cb6a90.png)
    
*   每个对象都可以成为锁，因为object类内置了ObjectMonitor对象（ObjectMonitor.java在open jdk中，是一个虚拟机专用对象 ），Object中的wait、notify等native方法都是依赖该对象  
    
    ![](/img/user/阅读库藏/assets/1661603816-2ac61bf7c0d326f8750b51f2e22c1f1d.png)
    
    ![](/img/user/阅读库藏/assets/1661603816-d956b2e4d7ae20ff82922264840543ab.png)
    
    *   调用hashcode方法后再进入同步块会导致偏向锁升级。因为一个对象的默认hashcode方法会导致哈希验证码存到对象头中，可偏向锁的标识机制刚好就依赖对象头中的哈希验证码的位置。因此，为了避免偏向锁升级，最好覆盖默认的hashcode方法实现，或者不要在同步代码块前后调用hashcode方法
*   线程等待和唤醒  
    *   synchronized、 wait、notify
		*   wait、notify必须在synchronized代码块成对的使用
		*   wait、notify是native方法  
    *   Lock.lock/unlock、Condition.await、Condition.signal
	*   Condition.await、Condition.signal是借由LockSupport实现的
		    *   LockSupport.park()、LockSupport.unpark()
		*   LockSupport是用来创建锁和其他同步类的基本线程阻塞原语。其使用了许可Permit这个概念来做到阻塞和唤醒线程的功能，每个线程都有一个许可，permit只有0和1两个值，默认是0，其实就是个信号量，类似于Semaphore，为1代表有许可，有许可则可继续执行。
		*   LockSupport更灵活(可唤醒指定线程，解耦同步锁与线程等待唤醒，unpark可在park前执行), 更有效率（无锁)
		*   LockSupport park/unpark主要借助unsafe类的本地方法park/unpark，方法最多本身额外的做了下记录  
	*   相对于wait/notify, Condition可以把等待/唤醒分类，避免“惊动整个群”(全部唤醒)  
	    *   前两者都要在加锁的情况下等待和唤醒，等待线程会释放锁，wait的话线程阻塞进入监视器等待队列WaitSet，notify的话线程恢复并移到监视器同步队列EntryList抢锁。Condition.await的话线程阻塞进入条件队列，Condition.signal的话线程恢复并移到Lock同步队列抢锁。

    
    ![](/img/user/阅读库藏/assets/1661603816-aa8cb747014ceb4e117c1c44f0a2bb0c.png)
    
    ![](/img/user/阅读库藏/assets/1661603816-a566153256b5315d04f250f869b83ded.png)
    
    ![](/img/user/阅读库藏/assets/1661603816-42c2aef7e1dc6ec03b623fba2b443dff.png)
    
    *   unsafe类的本地方法park/unpark，还依赖线程Thread内置的Parker对象
    
    ![](/img/user/阅读库藏/assets/1661603816-674982d9fd8201191004477a1787001e.png)
    
    *   park逻辑
    *   调用线程Parker对象的park(bool isAbsolute, jlong time)方法  
        
        *   原子性的设置许可为0，并取得前值
        
        ![](/img/user/阅读库藏/assets/1661603816-978a1c109951c028d54397b7eccef0b3.png)
        
        *   前值大于0即前值为1，则直接返回。这样相当于线程消费了许可，然后不阻塞继续执行了
        *   前值为0，继续往下走  
            
            *   若当前线程已经被中断，立即返回而不是抛出interrupt异常
            
            ![](/img/user/阅读库藏/assets/1661603816-64bb7f376462c448195e408b640b6b77.png)
            
            *   后面较为复杂，简单点说就是加锁后调os的pthread库把线程阻塞。也就是说线程如果拿不到许可就要阻塞
    *   unpark逻辑  
        *   首先加锁，然后设置\_counter=1,如果\_counter设置之前的值就为0，调os的pthread库唤醒等待的线程, 否则直接释放锁然后返回
    *   Java并发包下的RetreenLock实现
*   AQS  
    
    *   是JUC下同步器和锁等的基石框架类
    
    ![](/img/user/阅读库藏/assets/1661603816-150db09ff376fed21fa512e42be24951.png)
    
*   实现核心原理(队列、状态、CAS)  
    *   Lock：·
    *   CAS自旋抢同步状态  
        *   使用JDK的unsafe类的compareAndSwap方法
        *   这个方法是个native方法
        *   往下一层是c/c++语言的Atomic::cmpxchg
        *   再底层最终是调汇编指令lock  cmpxchg
    *   如果抢不到就记录线程到CLH同步队列，同时调用LockSupport提供的park方法将线程阻塞
    *   前面两步操作借由AQS封装好的东西来实现：  
        *   内部抽象类Sync继承自AQS队列，Sync下有两个实现的内部类FariSync与NonFariSync。这两个子类本质是队列，分别实现了公平锁与非公平锁的特性（公平与否是看是否先排队的可以先抢到锁）
    *   UNLOCK
	    *   从CLH队列中取一个线程，调用LockSupport提供的unpark方法将其唤醒
    *   Java并发包下的BlockingQueue实现
*   实现核心原理：  
    *   put
    *   get
    *   Lock的Condition接口
        *   每创建一个Condtion对象就会对应一个Condtion队列(条件)，每一个调用了Condtion对象的await方法的线程都会被包装成Node扔进一个条件队列中
        *   条件队列也是借由AQS封装好的东西来实现的
        *   等待锁的同步队列`sync queue`和条件队列`condition queue`是相互独立的，彼此之间并没有任何关系。但是，当我们调用某个条件队列的signal方法时，会将某个或所有等待在这个条件队列中的线程唤醒，被唤醒的线程和普通线程一样需要去争锁，如果没有抢到，则同样要被加到等待锁的`sync queue`中去，此时节点就从`condition queue`中被转移到`sync queue`中
    *   原子操作类
*   LongAdder具有高性能，功能符合的时候优先使用  
    *   如何做到高性能的？
    *   AtomicLong的基本原理是：内部long类型变量base记录累加值value，并发时多线程对base变量以CAS的方式进行增加。这也就决定了在并发竞争激烈时，有大量线程在自旋，白白的消耗cpu的资源，出现性能瓶颈。
    *   相比于AtomicLong，可以看出关键的原因在Striped64这个类  
        
        ![](/img/user/阅读库藏/assets/1661603816-f282a8bc44240c86372e6d3dd6d007b6.png)
        
    *   LongAdder解决性能瓶颈的思想是分治思想，以空间换时间，在高并发下获得更高的吞吐量  
        
        *   AtomicLong是base，而LongAdder是base + cell\[\]，Cell是Striped64的一个内部类
        
        ![](/img/user/阅读库藏/assets/1661603816-be02cf90a6cbdd25dd28246f84d5705b.png)
        
        *   基本的思路是分散热点，将value分散到cell数组中，不同线程会命中到数组的不同槽中，各线程只对自己槽中的那个分散值进行CAS操作，这样热点就分散了，发生CAS冲突而操作失败需要自旋的概率就减小了很多。如果要获取累加值value，只要把各个分散值累加返回即可
        *   LongAdder在无竞争的情况下，和AtomicLong一样对同一个base操作，当出现竞争关系时，就采用hash散列分散热点的做法
        
        ![](/img/user/阅读库藏/assets/1661603816-451ac7036f1fe240dcda9f526ce577a7.png)
        
        ![](/img/user/阅读库藏/assets/1661603816-6833afa64f6722b096a1133191093a69.png)
        
        ![](/img/user/阅读库藏/assets/1661603816-41c6846677f3de727f1d10357f38fd46.png)
        
    *   ThreadLocal
*   是啥![](/img/user/阅读库藏/assets/1661603816-5aa9355fdfdad0094e53cef4d15ec0b6.png)
*   JVM四大引用：强、软、弱、虚引用（回收优先级逐渐升高)  
    
    ![](/img/user/阅读库藏/assets/1661603816-15e4f2b763df8fcc43649f306e799ec8.png)
    
    *   [[Logseq元知识库/pages/强引用⁰\|强引用⁰]]
	    *   对JVM来说，设计含义为宁愿 #OutOfMemoryError⁰  也不会回收[[Logseq元知识库/pages/强引用⁰\|强引用⁰]]所指向的对象（宁死也不回收）
	    * [[Logseq元知识库/pages/强引用⁰\|强引用⁰]]的取消方法：显示的给引用变量赋值为null
	    *   [[Logseq元知识库/pages/强引用⁰\|强引用⁰]]对Java程序员来说就是最常见的普通对象的引用
    *   [[Logseq元知识库/pages/软引用⁰\|软引用⁰]]
	    *   对JVM来说，设计含义为[[Logseq元知识库/pages/软引用⁰\|软引用⁰]]所指向的对象 会在即将发生 #OutOfMemoryError⁰ 前才被回收
	    *   系统将要发生 #OutOfMemoryError⁰ 之前，将会把软引用对象列进回收范围进行第二次回收。如果这次回收还没有足够的内存，才会抛出 #OutOfMemoryError⁰ 
	    *   软引用对Java程序员来说就是java.lang.ref.SoftReference类实现的引用![](/img/user/阅读库藏/assets/1661603816-0088eb8494713ba868b6e9ebb3d9cde0.png)
    * [[Logseq元知识库/pages/弱引用⁰\|弱引用⁰]]
	    * 对于JVM来说，设计含义为[[Logseq元知识库/pages/弱引用⁰\|弱引用⁰]]所指向的对象只能存活到下一次  [[Logseq元知识库/pages/垃圾回收⁰\|垃圾回收⁰]]
	    * [[Logseq元知识库/pages/弱引用⁰\|弱引用⁰]]对我们来说就是java.lang.ref.WeakReference类实现的引用
        *   [[Logseq元知识库/pages/弱引用⁰\|弱引用⁰]]的应用场景
            * 举个例子；假如有一个应用需要读取大量本地图片，则可能碰上问题：  
                * 每次从硬盘读取IO太慢
                * 一次性放到内存缓存，又会造成内存溢出
                * 使用Map，key为图片路径，value为图片对象弱引用, 可以通过gc回收
    * [[Logseq元知识库/pages/虚引用⁰\|虚引用⁰]]  
        * 对JVM，设计含义为形同虚设，也就是JVM压根不认为[[Logseq元知识库/pages/虚引用⁰\|虚引用⁰]] 算一个“引用”。所以[[Logseq元知识库/pages/虚引用⁰\|虚引用⁰]] 无法影响对象的生命周期，如果一个对象只有[[Logseq元知识库/pages/虚引用⁰\|虚引用⁰]] ，那么它随时可能被回收
        *   [[Logseq元知识库/pages/虚引用⁰\|虚引用⁰]] 要java.lang.ref.PhantomReference实现
        *   [[Logseq元知识库/pages/虚引用⁰\|虚引用⁰]]基本只有搞JVM的应用到，且必须和引用队列Refrence Queue结合
        *   [[Logseq元知识库/pages/虚引用⁰\|虚引用⁰]]的具体作用就是跟踪对象被垃圾回收的状态。当虚引用指向对象要被回收，JVM除了调对象finalize方法，还会将虚引用保存到引用队列，通过这种方式提供通知，告知该对象被垃圾回收了
    *   Disruptor框架
    *   一个可在无锁的情况下实现并发操作的框架  
        *   典型场景：生产消费
        *   同其他队列(如ArrayBlockingQueue)对比:不是基于条件阻塞的，无锁，性能高
    *   核心设计原理  
        
        ![](/img/user/阅读库藏/assets/1661603816-44192915b5545440047bf5e5631f438e.png)
        
        *   环形数组结构
        *   RingBuffer是一个环形缓存区，入队的数据放在这里  
            *   RingBuffer内部维护一个数组entries，数据入队底层就是放到数组
            *   RingBuffer内部还维护了一个long类型递增序列sequence(long保证了序列够用，不会溢出)，新数据入队，要用一个该sequence的序列号对数组长度取模来计算索引，然后数据放到数组对应索引的位置
            *   因此，随着不停入队，数组从头到尾反复地被利用，成了一个环
            *   当出现入队覆盖的问题时，会暂停等数据消费
        *   位置定位
        *   数组长度为2的n次冥，这样sequence 位与 n-1即为入队索引。位于的效率高，所以定位块
        *   无锁设计